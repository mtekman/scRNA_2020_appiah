#+OPTIONS: :exports both
#+PROPERTY: header-args :exports both :eval never-export
#+SETUPFILE: https://fniessen.github.io/org-html-themes/setup/theme-readtheorg.setup
#+OPTIONS: H:4 num:nil toc:5
#+options: timestamp:nil
#+AUTHOR: Mehmet Tekman

#+TITLE: scVelo Analysis

Here we will use the matrices we obtained with the dropEst Pre-processing to perform an scVelo analysis

*  Testing scVelo

  Here and in [[file:testscvelo.ipynb][testscvelo.ipynb]] we run the toy dataset to establish how quick and easy the analysis is. 

**** SetupEnv

   #+NAME: setupenv
   #+begin_src python :session yes
     # load env
     activate_this_file = "/home/tetris/.scvelo/bin/activate_this.py"
     f=open(activate_this_file, 'r')
     exec(f.read(), dict(__file__=activate_this_file))
   #+end_src

   #+RESULTS: setupenv

**** Quick Start

  Here we will run the test pancreas dataset (this does not work on org, so I switch to ipynb after, please see the above test jupyter notebook.)

   #+begin_src python :session yes
     import scvelo as scv
     scv.set_figure_params()
     adata = scv.datasets.pancreas()
   #+end_src

   #+RESULTS:

   #+begin_src python :session yes
     scv.pp.filter_and_normalize(adata, **params)
     scv.pp.moments(adata, **params)
   #+end_src

   #+RESULTS:

**** Thoughts

     From an input dataset to a final labelled plot, the scvelo pipeline is fairly quick and straightforward to perform. The input dataset does require many annotations that you would not encounter in a normal Anndata dataset, so I am doubtful that this would be so easy on any matrix that has not been processed with DropEst/Velocyto beforehand.

* Test with RaceID Matrix and DropEst Matrix

  The full notebook of experimentation can be found [[file:Untitled.ipynb][here]], but below are some small snippets.

** RaceID Matrix

  This should not work, since it has not undergone the correct preprocessing, but I am curious

**** Convert RData to CSV

     Let's extract the experiment matrix and see if we can get any trajectories based on them

  #+begin_src R
    tab <- readRDS('~/Downloads/Injected_cells_batch2.RData')
    write.table(tab, file="Injected_cells_batch2.tsv", sep="\t", quote=F)
  #+end_src

** DropEst matrix

   This also did not work, because the output matrices had not yet had =velocyto tools= and then =velocyto run= performed on them. Doing this however, presented some problems:


* Velocyto on DropEST Matrix

  So the first preprocessing ran the required dropEST stages, but still needs to run velocyto to get the right transcripts.
  The [[http://velocyto.org/velocyto.py/tutorial/cli.html][online materials]] are also really outdated, referencing incorrect commands (e.g. =dropest_bc_correct= instead of =dropest-bc-correct=) and having more required parameters than the material online suggests (such as providing the exit RDS matrix).

  #+begin_src bash
   . ~/.scvelo/bin/activate
   cd ~/Downloads/bismark/2_kallisto_bams_70_05_overhang
   for bam in `find -type f -name "*.bam"`; do
       fname=../2a_corrected/$(echo $bam | sed -r 's|./([^.]+)\..*|\1|')
       mkdir -p $fname
       velocyto tools dropest-bc-correct $bam $fname
   done
  #+end_src

  #+RESULTS:

  Running this yielded nothing, because the RDS matrix is supposed to be BAM file specific (I think?) and maybe not an aggregation of several BAM files, as was performed in the previous DropEST analysis.

*** What does this mean?
    
    The BAM files should not be merged during the =dropEst= stage, and that maybe we should re-run our dropTag'd FASTQ files through =STAR= and not =Kallisto= to obtain the desired tags. 

* Re-Run dropTag BAM files on STAR

  This time we ran STAR on the tagged droptag reads as outlined [[http://velocyto.org/velocyto.py/tutorial/cli.html][in the materials]]. This time, we need a GTF file so I took the [[ftp://ftp.ensembl.org/pub/release-99/gtf/mus_musculus/Mus_musculus.GRCm38.99.gtf.gz][mm10 from Ensembl]]. 

  I uploaded the dropTag files to the Galaxy history /Bismark Tagged STAR Input Datasets/ and ran STAR on them.

  It is not entirely clear which reads they specify, since ours are paired end -- but the R1 reads are just barcodes and the barcode information we have already saved in external datasets, so I just mapped the R2 reads and achieved ~50-65% unique mapping rate, which is expected for CELseq2.

*** Download STAR BAM files from Galaxy

  Let's download these and run dropEst again.

  #+begin_src python 
     from bioblend import galaxy
     import os
     # Get this from https://usegalaxy.eu/user/api_key
     your_api_key = "BLAM"
     # Make sure this directory exists
     gi = galaxy.GalaxyInstance(url="https://usegalaxy.eu", key=your_api_key)
     dclient = galaxy.datasets.DatasetClient(gi)
     hl = gi.histories.get_histories()
     hclient = galaxy.histories.HistoryClient(gi)
     history = [lip for lip in hl if lip['name'] == "Bismark Tagged STAR Input Datasets"][0]
     tab = hclient.show_matching_datasets(history['id'], ".*\.bam")
     hid_name = {x['id']:x['name'] for x in tab}
     print("ID\tName")
     os.mkdir("bam_sets2")
     for hid in sorted(hid_name.keys()):
         print(hid, hid_name[hid], sep="\t")
         dclient.download_dataset(hid, file_path="%s/%s" % ("bam_sets2", hid_name[hid]),
                                  use_default_filename=False)
  #+end_src

  #+RESULTS:
  : None

  Here we obtain our BAM files, and now let's rename them to match the input FASTQ files.

*** Rename BAM files to match FASTQ inputs

  #+begin_src python :results output
    from bioblend import galaxy
    import os
    # Get this from https://usegalaxy.eu/user/api_key
    your_api_key = "BLAM"
    # Make sure this directory exists
    gi = galaxy.GalaxyInstance(url="https://usegalaxy.eu", key=your_api_key)
    dclient = galaxy.datasets.DatasetClient(gi)
    hl = gi.histories.get_histories()
    hclient = galaxy.histories.HistoryClient(gi)
    history = [lip for lip in hl if lip['name'] == "Bismark Tagged STAR Input Datasets"][0]
    tab = hclient.show_matching_datasets(history['id'], ".*\.2\.fastq\.gz")
    hid_name = {x['hid']:x['name'] for x in tab}
    for hid in sorted(hid_name.keys()):
        print(hid, hid_name[hid], sep="\t")
  #+end_src

  #+RESULTS:
  : 2	http://132.230.153.85:9099/aMI_DMSO_E14_RP13_R2.fastq.gz.tagged.2.fastq.gz
  : 4	http://132.230.153.85:9099/aMI_DMSO_E14_RP14_R2.fastq.gz.tagged.2.fastq.gz
  : 6	http://132.230.153.85:9099/aMI_DMSO_E14_RP15_R2.fastq.gz.tagged.2.fastq.gz
  : 8	http://132.230.153.85:9099/aMI_DMSO_E14_RP16_R2.fastq.gz.tagged.2.fastq.gz
  : 10	http://132.230.153.85:9099/aMI_EPZ_E14_RP1_R2.fastq.gz.tagged.2.fastq.gz
  : 12	http://132.230.153.85:9099/aMI_EPZ_E14_RP2_R2.fastq.gz.tagged.2.fastq.gz
  : 15	http://132.230.153.85:9099/aMI_EPZ_E14_RP4_R2.fastq.gz.tagged.2.fastq.gz

  Rename the BAM datasets based on the history IDs (done using =dired=)

  #+begin_src bash 
  ls ./bam_sets2/*
  #+end_src

  #+RESULTS:
  | ./bam_sets2/aMI_DMSO_E14_RP13_R2.bam | ./bam_sets2/aMI_EPZ_E14_RP1_R2.bam |
  | ./bam_sets2/aMI_DMSO_E14_RP14_R2.bam | ./bam_sets2/aMI_EPZ_E14_RP2_R2.bam |
  | ./bam_sets2/aMI_DMSO_E14_RP15_R2.bam | ./bam_sets2/aMI_EPZ_E14_RP4_R2.bam |
  | ./bam_sets2/aMI_DMSO_E14_RP16_R2.bam |                                    |

*** Regenerate CELseq2 XML

  Now let's regenerate the celseq2 xml

   #+begin_src xml :file ./celseq2_MPI.xml
     <config>
         <!-- droptag -->
         <TagsSearch>
             <protocol>cel_seq2</protocol>
             <MultipleBarcodeSearch>
                 <umi_start>0</umi_start>
                 <umi_length>6</umi_length>
                 <barcode_starts>6</barcode_starts>
                 <barcode_lengths>6</barcode_lengths>
             </MultipleBarcodeSearch>

             <Processing>
                 <min_align_length>10</min_align_length>
                 <reads_per_out_file>10000000</reads_per_out_file>
             </Processing>
         </TagsSearch>

         <!-- dropest -->
         <Estimation>
             <Merge>
                 <max_cb_merge_edit_distance>4</max_cb_merge_edit_distance>
                 <max_umi_merge_edit_distance>1</max_umi_merge_edit_distance>
                 <min_genes_after_merge>100</min_genes_after_merge>
                 <min_genes_before_merge>20</min_genes_before_merge>
             </Merge>
         </Estimation>
     </config>

   #+end_src

*** Run DropEST

   and then we run DropEST

   #+begin_src bash :session new
     . /home/tetris/.scvelo/bin/activate
     export PATH=$PATH:/opt/dropest/bin/
     outdir=dropest/test/
     mkdir -p $outdir
     dropest -m -V -b -o dropest/test/ \
             -g /home/tetris/repos/_work/_scrna/scRNA_2020_appiah/Mus_musculus.GRCm38.99.gtf \
             -L eiEIBA \
             -c /home/tetris/repos/_work/_scrna/scRNA_2020_appiah/celseq2_MPI.xml \
             /home/tetris/repos/_work/_scrna/scRNA_2020_appiah/bam_sets2/aMI_DMSO_E14_RP13_R2.bam
   #+end_src

   and we immediately get errors of =ERROR: unable to parse out UMI in: KCGN14909114= and this repeats on and on likely for all reads.

   Why can it not extract the UMI? Let's have a look

**** Inspect a BAM file

   #+begin_src bash
     samtools view /home/tetris/repos/_work/_scrna/scRNA_2020_appiah/bam_sets2/aMI_DMSO_E14_RP13_R2.bam | head
   #+end_src

   #+RESULTS:
   | KCGN13282784 | 16 | chr1 | 3010577 | 60 | 70M   | * | 0 | 0 | GAGGTTAGCAATCTAGGGTCAGATAATTATAGGTCTAGGTGCTGATTTCTGAGTTTTTGTTGGATGGGCG | JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJ<AFA7<<A | NH:i:1 | HI:i:1 | AS:i:68 | nM:i:0 |
   | KCGN12771152 | 16 | chr1 | 3010578 | 60 | 70M   | * | 0 | 0 | AGGTTAGCAATCTAGGGTCAGATAATTATAGGTCTAGGTGCTGATTTCTGAGTTTTTGTTGGATGGGCGT | JJJJJJJJJJJJJJJJJJJFJAJ<JJJJJJFFJJJJJJFJJJJJJJJJJFAJJJJJJJFJA<-F7-7-<A | NH:i:1 | HI:i:1 | AS:i:68 | nM:i:0 |
   | KCGN11152450 | 16 | chr1 | 3010646 | 60 | 70M   | * | 0 | 0 | GGTTTGTTTGTTTTTTTTGTTTGTTTGTTTGTTTTTTCTGTTTCCTTTTCAGTTTTTTGGCTTTTGTGAT | JJJJJJJJJJJJJJJJJJJJJJFJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJFFJJJJJJJFAAF<A | NH:i:1 | HI:i:1 | AS:i:68 | nM:i:0 |
   | KCGN11152487 | 16 | chr1 | 3010652 | 60 | 70M   | * | 0 | 0 | GTTTTTGGTTTGTTTGTTTTTTTTGTTTGTTTGTTTGTTTTTTCTGTTTCCTTTTCAGTTTTTTGGCTTT | FJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJFFJJFJJFF<<A | NH:i:1 | HI:i:1 | AS:i:68 | nM:i:0 |
   | KCGN11152541 | 16 | chr1 | 3010680 | 60 | 70M   | * | 0 | 0 | TTGTTTTTTCTGTTTCCTTTTCAGTTTTTTGGCTTTTGTGATCTGGATTTTTGATGGCTATCATGACCTC | FJJJJJJJFJJJJFJFJJJJJJJJJJJJJJFJJJJJFFJFJJJJJJJJJJJJJJJJAJJJJJJF7-77<- | NH:i:1 | HI:i:1 | AS:i:68 | nM:i:0 |
   | KCGN11152627 | 16 | chr1 | 3010714 | 60 | 69M1S | * | 0 | 0 | TTTGTGATCTGGATTTTTGATGGCTATCATGACCTCTGAATGACTAGGGAATCTTGGACCACATTGGGGA | JJJJJJJJJJJJJJJJJJJJJJFFJFJFJJA<JJJJJJJJJJFJJFJFJJJJJJJJJ-FJAJJJJF<<<A | NH:i:1 | HI:i:1 | AS:i:67 | nM:i:0 |
   | KCGN11500167 | 16 | chr1 | 3010715 | 60 | 70M   | * | 0 | 0 | TTTTGATCTGGATTTTTGCTGGCTATCATGACCTCTGAGTGACTAGCGAATCTTGGCCCACATTGGGGGC | JF-AFAA<-F-------7-JF7-7JFJF-F<7A--JF<-<-<-<-<-<--F7<-77----AA<F--<<-- | NH:i:1 | HI:i:1 | AS:i:58 | nM:i:5 |
   | KCGN11152468 | 16 | chr1 | 3010727 | 60 | 70M   | * | 0 | 0 | TTTTTGATGGCTATCATGACCTCTGAATGACTAGGGAATCTTGGACCACATTGGGGGCCTCTACTAGCTG | JJJJJJJJJJFJJJJJJJJJJJJJJFJFJJAFAJJJJJJJJJJFFJFJJJJJJFJAJ<JJJJFJJF77<A | NH:i:1 | HI:i:1 | AS:i:68 | nM:i:0 |
   | KCGN11500219 | 16 | chr1 | 3010740 | 60 | 67M3S | * | 0 | 0 | TCATGACCTCTGAATGACTAGGGAATCTTGGACCACATTGGGGGCCTCTACTAGCTGTTAGCTTGGCCTC | JJJJJJJJJJJJJJJJJJJJJJFJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJFJJJA7<<<A | NH:i:1 | HI:i:1 | AS:i:65 | nM:i:0 |
   | KCGN11152560 | 16 | chr1 | 3010755 | 60 | 69M1S | * | 0 | 0 | GACTAGGGAATCTTGGACCACATTGGGGGCCTCTACTAGCTGTTAGCTTGGCATTTGGTACCATGCGGGC | JFJJFJJJJJJJJJJJJJJJJJJJJJJFJJJJJJJJJJJJJJJJFJJJJJJJJJJJJJJJJFJJJ<A<<- | NH:i:1 | HI:i:1 | AS:i:67 | nM:i:0 |

   It could be that since they were mapped in STAR using only the R2 reads, the barcoding info (in R1 reads) are lost. I assumed  that it could recuperate this info from the external data, but apparently not.
  
*** Run DropEST with Params file

   Let's try with the =-r= flag which we can feed the params file into.

   #+begin_src bash
     dropest -m -V -b -o dropest/test/ \
             -r  /home/tetris/Downloads/bismark/1_processed_by_droptag/aMI_DMSO_E14_RP13_R2.fastq.gz.tagged.params.gz \
             -g /home/tetris/repos/_work/_scrna/scRNA_2020_appiah/Mus_musculus.GRCm38.99.gtf \
             -L eiEIBA \
             -c ls d/home/tetris/repos/_work/_scrna/scRNA_2020_appiah/celseq2_MPI.xml \
             /home/tetris/repos/_work/_scrna/scRNA_2020_appiah/bam_sets2/aMI_DMSO_E14_RP13_R2.bam
   #+end_src

   So this time, the errors are parsed correctly, but we get =WARNING: Can't find chromosome 'chr1'= for all chromosomes, where it later tells us that it cant find any cells above acceptable thresholds.

   So what do we do? We change the GTF file so that every data line is prepended with 'chr', this will work flawlessly with all canonical chromosomes but we will see some small errors for less well defined chromosome such as =chrUn_*_random=


*** Run DropEST with Params file and 'chr'-prefixed GTF file


   #+begin_src bash :session new
     #note change in GTF
     dropest -m -V -b -o dropest/test/out \
             -r  /home/tetris/Downloads/bismark/1_processed_by_droptag/aMI_DMSO_E14_RP13_R2.fastq.gz.tagged.params.gz \
             -g /home/tetris/repos/_work/_scrna/scRNA_2020_appiah/Mus_musculus.GRCm38.99.chr.gtf \
             -L eiEIBA \
             -c ls d/home/tetris/repos/_work/_scrna/scRNA_2020_appiah/celseq2_MPI.xml \
             /home/tetris/repos/_work/_scrna/scRNA_2020_appiah/bam_sets2/aMI_DMSO_E14_RP13_R2.bam
   #+end_src

   This time we actually got most of our reads to align fine, with 17.9% intergenic, 50.1% touch exon, 23% touch intron, 1.61% touch both gene and not annotated regions, can't parse 9.58667% reads, ), 0 non-mapped reads, 3865 CBs read. So we lost 10% of our reads but this might actually be fine for analysis. It even detected 671 cells for this batch alone (if that is accurate?)

   Let us see if we can complete the rest of this hacky workflow.

*** Performing the rest of the analysis

   #+begin_src bash :session new
     velocyto tools dropest-bc-correct /home/tetris/repos/_work/_scrna/scRNA_2020_appiah/bam_sets2/aMI_DMSO_E14_RP13_R2.bam dropest/test/out.rds
   #+end_src

   Edit: nope! We get the same error that we got in the previous workflow:

   =UnboundLocalError: local variable 'convert_r_obj' referenced before assignmen=

   where this error is referenced here: https://github.com/velocyto-team/velocyto.py/issues/224

   I managed to [[https://github.com/velocyto-team/velocyto.py/issues/224][submit a bug report]] and fix this error (it was missing a specific version of rpy2), but then it threw errors relating to not being able to find the CB tag in the BAM file for the cell barcodes.

   #+begin_src 
     velocyto run-dropest -b /home/tetris/Downloads/bismark/1_processed_by_droptag/aMI_DMSO_E14_RP13_R2.fastq.gz.tagged.rds -o dropest/test_results /home/tetris/repos/_work/_scrna/scRNA_2020_appiah/bam_sets2/aMI_DMSO_E14_RP13_R2.bam ./Mus_musculus.GRCm38.99.chr.gtf
   #+end_src

*** What to do

   So for some reason our BAM file is not tagged, and to be honest -- why would it be? We have given no special commands to STAR during the alignment, so I assumed it would rely on external files such as those given in the =tagged.params.gz= or =tagged.rds= which are output from =dropTAG=.

   My next guess would then be that the =dropTAG= step was either not performed correctly, or that the relevant metadata (such as barcodes) is not passed into the =dropest-bc-correct= or the =run-dropest= commands properly.


** Note

 At this point two issues started to become apparent:

*** 1. The current tools and their materials are mismatched

    The tool names given in the training material are outdated, and the newer tools require more mandatory arguments that the material makes no reference to.

    The alignment and quantification steps are not clear either as some steps prefer the use of alignment-free tools such as Kallisto (as was performed previously), and others prefer the use of STAR.

*** 2. The tools are badly written 

    Why does a Python tool (=velocyto=) need to write out RData matrices? Especially when the next stage of analysis is again a Python library (=scvelo=)? 

    I also ran into several problems trying to get the tools running, and submitted a [[https://github.com/velocyto-team/velocyto.py/issues/224][bug report]] to the team to actually list their dependencies or to provide a conda environment that is not broken. DropEst itself had to be built from scratch to actually get it to work.
  

* How to proceed

  This pipeline is mess, and this is saying something as I am quite used to working with messy pipelines. 

  I believe a full pre-processing would need to be performed so that the =dropTAG= to =STAR= steps produce the required tags in the BAM file. I imagine this will take a lot of trial and error since the materials do not seem to reference this issue but [[https://github.com/velocyto-team/velocyto.py/issues/251][I am not alone in this problem]] as one of the many many frustrated users of this pipeline.

  The above and related notebooks are the output of 1-2 days worth of work. If I work on this solidly for a week I might be able to get this working.
